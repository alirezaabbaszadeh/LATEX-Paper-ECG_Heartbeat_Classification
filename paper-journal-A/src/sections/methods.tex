\section{Materials and Methods}
\subsection{Datasets}
We conducted experiments on the MIT-BIH Arrhythmia Database distributed via PhysioNet, comprising 48 half-hour, two-lead ambulatory ECG recordings sampled at 360~Hz with 110{,}919 annotated beats across the AAMI classes (N, S, V, F, Q)\cite{goldberger2000physiobank,moody2001impact}. Following the repository default, we reserved 20\% of records for a stratified test set and performed five-fold cross-validation on the remaining subjects before retraining on the full training set for final evaluation. Data were obtained programmatically with \texttt{download\_data.py}, which retrieves the original signal files and annotations into a local cache while enabling auditing steps that mirror recent recommendations for detecting acquisition bias in machine learning-ready ECG corpora\cite{morchpedersen2024unraveling}.

\subsection{Preprocessing}
Beat-centric representations were generated with \texttt{preprocess\_data.py}. The script detects R-peaks, crops 187-sample windows centred on each annotated beat, and transforms each segment into a complex Morlet wavelet scalogram while recording metadata that links beats to source records and AAMI labels. R-peak localisation combines amplitude and slope cues reminiscent of classical detectors\cite{pan1985real,laguna1994automatic}, whereas the Morlet scalogram emphasises multiresolution time-frequency structure well suited to arrhythmia morphology\cite{addison2005wavelet,torrence1998practical,mallat1989wavelet}. Preserving phase and directional information in this representation supports downstream interpretation of repolarisation dispersion and atrial activity patterns that Journal of Electrocardiology studies have linked to cardiovascular risk stratification\cite{ozderya2024relationship,sbrollini2024directionalities,isabelroquero2024longitudinal,li2024deep,wang2024construction,liu2024association}. To avoid leakage, amplitude normalisation is deferred until dataset construction; only raw scalograms and metadata are written to \texttt{preprocessed\_data\_h5\_raw/}. Sequences of three temporally adjacent beats are then assembled and serialised as batched TFRecords via \texttt{create\_batched\_tfrecords.py}, which also embeds tensor shapes, sequence length, and batch size to facilitate streaming ingestion.

\subsection{Model Design}
The primary model is implemented in \texttt{ModelBuilder.py} as a CNN-Conformer hybrid. A stack of two-dimensional convolutions first extracts local wavelet patterns, after which multiple Conformer blocks---comprising macaron feed-forward modules, multi-head self-attention with relative positional encodings, and depthwise separable convolutions---model long-range temporal context\cite{vaswani2017attention}. Residual connections, dropout, and layer normalisation stabilise training, drawing on widely adopted regularisation and normalisation strategies for deep networks\cite{he2016deep,srivastava2014dropout,ba2016layer} and aligning with Journal of Electrocardiology reports on residual-attention classifiers and Siamese retrieval models for arrhythmia triage\cite{rahman2024residual,an2024fewshot}. A final dense classifier outputs posterior probabilities over the five AAMI classes. Comparator models include an attention-only variant that removes the convolution branch and a CNN-LSTM baseline that substitutes recurrent units for the Conformer encoder, enabling controlled ablations of architectural components.

\subsection{Training and Evaluation}
Training pipelines are orchestrated by \texttt{run\_hyperparameter\_tuning.py}, \texttt{run\_kfold\_evaluation.py}, and \texttt{run\_final\_evaluation.py}. Hyperparameter tuning employs Hyperband with deterministic TensorFlow operations and fold-wise normalisation statistics sourced from \texttt{MainClass.py}\cite{li2018hyperband}. For each fold, \texttt{DataLoader.py} streams TFRecord batches with interleaved shuffling, on-the-fly normalisation, and optional augmentation. The final configuration is retrained on the union of training folds and evaluated on the held-out test split using Adam optimisation\cite{kingma2015adam} with cosine-annealed learning-rate schedules. \texttt{Evaluator.py} emits classification reports, confusion matrices, and ROC/precision-recall curves to balance overall discrimination with rare-class sensitivity\cite{fawcett2006introduction,saito2015precision}. Primary metrics include accuracy, macro- and weighted-F1, per-class area under the ROC curve (AUC), and cross-validation means with standard deviations to characterise variability across subjects.
