\section{Materials and Methods}
\subsection{Datasets}
We conducted experiments on the MIT-BIH Arrhythmia Database distributed via PhysioNet, comprising 48 half-hour, two-lead ambulatory ECG recordings sampled at 360~Hz with 110{,}919 annotated beats across the AAMI classes (N, S, V, F, Q)\cite{goldberger2000physiobank}. Following the repository default, we reserved 20\% of records for a stratified test set and performed five-fold cross-validation on the remaining subjects before retraining on the full training set for final evaluation. Data were obtained programmatically with \texttt{download\_data.py}, which retrieves the original signal files and annotations into a local cache.

\subsection{Preprocessing}
Beat-centric representations were generated with \texttt{preprocess\_data.py}. The script detects R-peaks, crops 187-sample windows centred on each annotated beat, and transforms each segment into a complex Morlet wavelet scalogram while recording metadata that links beats to source records and AAMI labels. To avoid leakage, amplitude normalisation is deferred until dataset construction; only raw scalograms and metadata are written to \texttt{preprocessed\_data\_h5\_raw/}. Sequences of three temporally adjacent beats are then assembled and serialised as batched TFRecords via \texttt{create\_batched\_tfrecords.py}, which also embeds tensor shapes, sequence length, and batch size to facilitate streaming ingestion.

\subsection{Model Design}
The primary model is implemented in \texttt{ModelBuilder.py} as a CNN-Conformer hybrid. A stack of two-dimensional convolutions first extracts local wavelet patterns, after which multiple Conformer blocks---comprising macaron feed-forward modules, multi-head self-attention with relative positional encodings, and depthwise separable convolutions---model long-range temporal context. Residual connections, dropout, and layer normalisation stabilise training, and a final dense classifier outputs posterior probabilities over the five AAMI classes. Comparator models include an attention-only variant that removes the convolution branch and a CNN-LSTM baseline that substitutes recurrent units for the Conformer encoder, enabling controlled ablations of architectural components.

\subsection{Training and Evaluation}
Training pipelines are orchestrated by \texttt{run\_hyperparameter\_tuning.py}, \texttt{run\_kfold\_evaluation.py}, and \texttt{run\_final\_evaluation.py}. Hyperparameter tuning employs Hyperband with deterministic TensorFlow operations and fold-wise normalisation statistics sourced from \texttt{MainClass.py}. For each fold, \texttt{DataLoader.py} streams TFRecord batches with interleaved shuffling, on-the-fly normalisation, and optional augmentation. The final configuration is retrained on the union of training folds and evaluated on the held-out test split, with \texttt{Evaluator.py} emitting classification reports, confusion matrices, and ROC/precision-recall curves. Primary metrics include accuracy, macro- and weighted-F1, per-class area under the ROC curve (AUC), and cross-validation means with standard deviations to characterise variability across subjects.
